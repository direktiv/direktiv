# -- Container registry from which to pull direktiv.
registry: "docker.io"

# -- Container pull policy.
pullPolicy: Always

# -- Container registry secrets.
imagePullSecrets: []

# -- image for main direktiv binary
image: "direktiv/direktiv"

# -- image tag for main direktiv binary pod
tag: ""

# -- enabled api key for API authentication with the `direktiv-token` header
apikey: none

# -- max request timeouts in seconds. Used in Knative and the ingress controller if enabled.
requestTimeout: 7200

nodeSelector: {}
tolerations: []
affinity: {}

flow:
  # -- Output debug-level logs.
  debug: false
  # -- Set to define an encryption key to be used for secrets. If set to empty, one will be generated on install.
  encryptionKey:
  # -- number of flow replicas
  replicas: 1
  # -- extra environment variables in flow pod
  extraVariables:
    []
    # - name:
    #   value:
  # -- extra container in flow pod
  extraContainers: []
  # -- extra volume mounts in flow pod
  extraVolumeMounts:
    # - name: service-template
    #   mountPath: /etc/config
  # -- extra volumes in flow pod
  extraVolumes:
    # - name: service-template
    #   configMap:
    #     name: service-template
  # -- affinity for flow pods
  affinity:
    {}
    # podAntiAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #   - labelSelector:
    #       matchExpressions:
    #       - key: app.kubernetes.io/name
    #         operator: In
    #         values:
    #         - direktiv
    #     topologyKey: kubernetes.io/hostname
  containers:
    secrets:
      resources:
        requests:
          memory: "128Mi"
        limits:
          memory: "512Mi"

  # -- Knative max scale
  max_scale: 5
  sidecar:

database:
  # -- database host
  host: "postgres-postgresql-ha-pgpool.postgres"
  # -- database port
  port: 5432
  # -- database user
  user: "direktiv"
  # -- database password
  password: "direktivdirektiv"
  # -- database name, has to be created before installation
  name: "direktiv"
  # -- sslmode for database
  sslmode: require
  # -- additional connection attributes, e.g. target_session_attrs
  additional: ""

# Added this to connect to tracing.direktiv.io
opentelemetry:
  enabled: false
  image: otel/opentelemetry-collector:latest
  agentconfig: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    exporters:
      otlp/traces:
        endpoint: direktiv-data-prepper.default.svc:21890
        tls:
          insecure: true
      otlp/data-prepper:
        endpoint: direktiv-data-prepper.default.svc:21891
        tls:
          insecure: true
      otlp/logs:
        endpoint: direktiv-data-prepper.default.svc:21892
        tls:
          insecure: true
    service:
      pipelines:
        traces: 
          receivers: [otlp]
          exporters: [otlp/traces]
        metrics:
          receivers: [otlp]
          exporters: [otlp/data-prepper]
        logs:
          receivers: [otlp]
          exporters: [otlp/logs]

ingress:
  # --
  enabled: true
  # -- Host for external services, only required for TLS
  host:
  # -- TLS secret
  certificate:
  # -- Ingress class
  class: "nginx"
  # -- Additional Annotations
  additionalAnnotations: {}
  # -- Additional Labels
  additionalLabels: {}

# -- nginx ingress controller configuration
ingress-nginx:
  install: true
  controller:
    podAnnotations:
      linkerd.io/inject: disabled
    config:
      proxy-buffer-size: "16k"
    replicaCount: 1
    admissionWebhooks:
      patch:
        podAnnotations:
          linkerd.io/inject: disabled

fluent-bit:
  install: true
  envFrom:
    - secretRef:
        name: direktiv-fluentbit
  config:
    inputs: |
      [INPUT]
          Name                    tail
          Path                    /var/log/containers/*flow*.log,/var/log/containers/*direktiv-sidecar*.log
          Mem_Buf_Limit           5MB
          Skip_Long_Lines         Off
          Tag                     input
          multiline.parser        cri, docker
          Refresh_Interval        1
          Buffer_Max_Size         64k
    outputs: |
      [OUTPUT]
          name                    pgsql
          match                   flow.*
          port                    ${PG_PORT}
          table                   fluentbit
          user                    ${PG_USER}
          database                ${PG_DB_NAME}
          host                    ${PG_HOST}
          password                ${PG_PASSWORD}
    filters: |
      [FILTER]
          Name                    rewrite_tag
          Match                   input
          Rule                    $log ^.*"track":"([^"]*).*$ flow.$1 true
      [FILTER]
          Name                    parser
          Match                   *
          Parser                  json
          Key_Name                log
          Reserve_Data            on
# [OUTPUT]
#     Name                    http
#     Match                   *
#     Host                    direktiv-data-prepper.default.svc
#     Port                    2021
#     URI                     /log/ingest
#     Format                  json
#     Retry_Limit             False
#     tls                     Off
# -- service account for components. If preconfigured serviceaccounts are used the name ise the base
# and two additional service accounts are needed, e.g. service account name is myaccount then another two
# acounts are needed: myaccount-functions and myaccount-functions-pod
serviceAccount:
  annotations: {}
  name: ""
  create: true
  # example to annotate for GCP database access
  #   annotations:
  #      iam.gke.io/gcp-service-account: IAM_USER@GCP_PROJECT.iam.gserviceaccount.com

# -- http proxy settings
http_proxy: ""
# -- https proxy settings
https_proxy: ""
# -- no proxy proxy settings
no_proxy: ""

functions:
  # -- knative service limits
  limits:
    memory:
      small: 512
      medium: 1024
      large: 2048
    cpu:
      small: 250m
      medium: 500m
      large: 1
    disk:
      small: 256
      medium: 1024
      large: 4096

  # namespace to run functions in
  namespace: direktiv-services-direktiv
  ingressClass: contour.ingress.networking.knative.dev

  # -- number of controller replicas
  replicas: 1

  # -- Egress/Ingress network limit for functions if supported by network
  netShape:

  # -- Cleaning up tasks, Kubernetes < 1.20 does not clean finished tasks
  podCleaner: true # deprecated

  # -- runtime to use, e.g. gvisor on GCP
  runtime: "default"

  affinity: {}

  # -- extra containers for function controller, e.g. database containers for google cloud or logging
  extraContainersPod: []

  # -- extra volumes for tasks and knative pods
  extraVolumes: []
  #   - configMap:
  #   name: otel-agent-conf
  #   items:
  #     - key: otel-agent-config
  #       path: otel-agent-config.yaml
  # name: otel-agent-config-vol

  # -- extra containers for tasks and knative pods
  extraContainers:
    []
    # - name: cloud-sql-proxy
    #   image: gcr.io/cloudsql-docker/gce-proxy:1.17
    #   command:
    #     - "/cloud_sql_proxy"
    #     - "-instances=mygcpdb=tcp:5432"
    #     - "-ip_address_types=PRIVATE"
    #   securityContext:
    #     runAsNonRoot: true
    #   resources:
    #     requests:
    #       memory: "2Gi"
    #       cpu:    "1"

nats:
  install: false
  config:
    cluster:
      enabled: true
      port: 6222
      replicas: 3
    tls:
      enabled: false
      secretName:
      dir: /etc/nats-certs/cluster
      cert: tls.crt
      key: tls.key
    routeURLs:
      user: direktiv
      password: direktiv
      useFQDN: false
      k8sClusterDomain: cluster.local
    nats:
      port: 4222
      tls:
        enabled: false

opensearch:
  install: false
  clusterName: "direktiv-opensearch-cluster"
  persistence:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 8Gi
    storageClassName: "local-path"
  services:
    enviroment:
  extraEnvs:
    - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
      value: direktivTr0&3
    - name: "ES_JAVA_OPTS"
      value: "Xms4g -Xmx4g"
  plugins:
    security:
      ssl_cert_reload_enabled: true
      ssl:
        transport:
          pemcert_filepath: esnode.pem
          pemkey_filepath: esnode-key.pem
          pemtrustedcas_filepath: root-ca.pem
          enforce_hostname_verification: false
        http:
          enabled: false
      allow_unsafe_democertificates: true

  protocol: https
  httpPort: 9200

data-prepper:
  enabled: true
  config:
    data-prepper-config.yaml: |
      ssl: false
      circuit_breakers:
        heap:
          usage: 2gb
          reset: 30s
          check_interval: 5s
  pipelineConfig:
    enabled: true
    config:
      otel-logs-pipeline:
        workers: 5
        delay: 10
        source:
          otel_logs_source:
            ssl: false
        buffer:
          bounded_blocking:
        sink:
          - opensearch:
              hosts: ["https://opensearch-cluster-master.default.svc:9200"]
              insecure: true
              username: admin
              password: direktivTr0&3
              index_type: custom
              index: events
              # index: events-%{yyyy.MM.dd}
              #max_retries: 20
              bulk_size: 4
      otel-trace-pipeline:
        # workers is the number of threads processing data in each pipeline.
        # We recommend same value for all pipelines.
        # default value is 1, set a value based on the machine you are running Data Prepper
        workers: 8
        # delay in milliseconds is how often the worker threads should process data.
        # Recommend not to change this config as we want the otel-trace-pipeline to process as quick as possible
        # default value is 3_000 ms
        delay: "100"
        source:
          otel_trace_source:
            ssl: false # Change this to enable encryption in transit
        buffer:
          bounded_blocking:
            # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory.
            # We recommend to keep the same buffer_size for all pipelines.
            # Make sure you configure sufficient heap
            # default value is 12800
            buffer_size: 25600
            # This is the maximum number of request each worker thread will process within the delay.
            # Default is 200.
            # Make sure buffer_size >= workers * batch_size
            batch_size: 400
        sink:
          - pipeline:
              name: "raw-traces-pipeline"
          - pipeline:
              name: "otel-service-map-pipeline"
      raw-traces-pipeline:
        workers: 5
        delay: 3000
        source:
          pipeline:
            name: "otel-trace-pipeline"
        buffer:
          bounded_blocking:
            buffer_size: 25600 # max number of records the buffer accepts
            batch_size: 400 # max number of records the buffer drains after each read
        processor:
          - otel_trace_raw:
          - otel_trace_group:
              hosts: ["https://opensearch-cluster-master.default.svc:9200"]
              insecure: true
              username: admin
              password: direktivTr0&3
        sink:
          - opensearch:
              hosts: ["https://opensearch-cluster-master.default.svc:9200"]
              insecure: true
              username: admin
              password: direktivTr0&3
              index_type: trace-analytics-raw
      otel-service-map-pipeline:
        workers: 5
        delay: 3000
        source:
          pipeline:
            name: "otel-trace-pipeline"
        processor:
          - service_map_stateful:
              # The window duration is the maximum length of time the data prepper stores the most recent trace data to evaluvate service-map relationships.
              # The default is 3 minutes, this means we can detect relationships between services from spans reported in last 3 minutes.
              # Set higher value if your applications have higher latency.
              window_duration: 180
        buffer:
          bounded_blocking:
            # buffer_size is the number of ExportTraceRequest from otel-collector the data prepper should hold in memeory.
            # We recommend to keep the same buffer_size for all pipelines.
            # Make sure you configure sufficient heap
            # default value is 12800
            buffer_size: 25600
            # This is the maximum number of request each worker thread will process within the delay.
            # Default is 200.
            # Make sure buffer_size >= workers * batch_size
            batch_size: 400
        sink:
          - opensearch:
              hosts: ["https://opensearch-cluster-master.default.svc:9200"]
              insecure: true
              username: admin
              password: direktivTr0&3
              index_type: trace-analytics-service-map
              #index: otel-v1-apm-span-%{yyyy.MM.dd}
              #max_retries: 20
              bulk_size: 4
      otel-metrics-pipeline:
        workers: 8
        delay: 3000
        source:
          otel_metrics_source:
            health_check_service: true
            ssl: false
        buffer:
          bounded_blocking:
            buffer_size: 1024 # max number of records the buffer accepts
            batch_size: 1024 # max number of records the buffer drains after each read
        processor:
          - otel_metrics:
              calculate_histogram_buckets: true
              calculate_exponential_histogram_buckets: true
              exponential_histogram_max_allowed_scale: 10
              flatten_attributes: false
        sink:
          - opensearch:
              hosts: ["https://opensearch-cluster-master.default.svc:9200"]
              insecure: true
              username: admin
              password: direktivTr0&3
              index_type: custom
              # index: metrics-%{yyyy.MM.dd}
              index: metrics

              #max_retries: 20
              bulk_size: 4
