# -- Container registry from which to pull direktiv.
registry: "docker.io"

# -- Container pull policy.
pullPolicy: Always

# -- Container registry secrets.
imagePullSecrets: []

# -- image for main direktiv binary
image: "direktiv/direktiv"

# -- image tag for main direktiv binary pod
tag: ""

# -- enabled api key for API authentication with the `Direktiv-Api-Key` header
apikey: none

# -- max request timeouts in seconds. Used in Knative and the ingress controller if enabled.
requestTimeout: 7200

nodeSelector: {}
tolerations: []
affinity: {}

flow:
  # -- Output debug-level logs.
  debug: false
  # -- Set to define an encryption key to be used for secrets. If set to empty, one will be generated on install.
  encryptionKey:
  # -- number of flow replicas
  replicas: 1
  # -- extra environment variables in flow pod
  extraVariables:
    []
    # - name:
    #   value:
  # -- extra container in flow pod
  extraContainers: []
  # -- extra volume mounts in flow pod
  extraVolumeMounts:
    # - name: service-template
    #   mountPath: /etc/config
  # -- extra volumes in flow pod
  extraVolumes:
    # - name: service-template
    #   configMap:
    #     name: service-template
  # -- affinity for flow pods
  affinity:
    {}
    # podAntiAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #   - labelSelector:
    #       matchExpressions:
    #       - key: app.kubernetes.io/name
    #         operator: In
    #         values:
    #         - direktiv
    #     topologyKey: kubernetes.io/hostname
  containers:
    secrets:
      resources:
        requests:
          memory: "128Mi"
        limits:
          memory: "512Mi"

  # -- Knative max scale
  max_scale: 5
  opentelemetryBackend: ""
  sidecar:

otel:
  install: true
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    processors:
      batch:
    exporters:
      otlp/tempo:
        endpoint: "tempo:4317"   
        tls:
          insecure: true          
      debug:
        verbosity: detailed
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [debug, otlp/tempo]

opentelemetry-collector:
  mode: deployment
  config:
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    processors:
      batch:
    exporters:
      logging:
        loglevel: debug   # prints spans to stdout
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [logging]

# opentelemetry-collector:
#   image:
#     repository: otel/opentelemetry-collector-k8s
#   mode: deployment

database:
  # -- database host
  host: "postgres-postgresql-ha-pgpool.postgres"
  # -- database port
  port: 5432
  # -- database user
  user: "direktiv"
  # -- database password
  password: "direktivdirektiv"
  # -- database name, has to be created before installation
  name: "direktiv"
  # -- sslmode for database
  sslmode: require
  # -- additional connection attributes, e.g. target_session_attrs
  additional: ""

ingress:
  # --
  enabled: true
  # -- Host for external services, only required for TLS
  host:
  # -- TLS secret
  certificate:
  # -- Ingress class
  class: "nginx"
  # -- Additional Annotations
  additionalAnnotations: {}
  # -- Additional Labels
  additionalLabels: {}

# -- nginx ingress controller configuration
ingress-nginx:
  install: true
  controller:
    podAnnotations:
      linkerd.io/inject: disabled
    config:
      proxy-buffer-size: "16k"
    replicaCount: 1
    admissionWebhooks:
      patch:
        podAnnotations:
          linkerd.io/inject: disabled

fluent-bit:
  existingConfigMap: config-fluentbit
  config:
    outputs: ""

# -- service account for components. If preconfigured serviceaccounts are used the name ise the base
# and two additional service accounts are needed, e.g. service account name is myaccount then another two
# acounts are needed: myaccount-functions and myaccount-functions-pod
serviceAccount:
  annotations: {}
  name: ""
  create: true
  # example to annotate for GCP database access
  #   annotations:
  #      iam.gke.io/gcp-service-account: IAM_USER@GCP_PROJECT.iam.gserviceaccount.com

# -- http proxy settings
http_proxy: ""
# -- https proxy settings
https_proxy: ""
# -- no proxy proxy settings
no_proxy: ""

functions:
  # -- knative service limits
  limits:
    memory:
      small: 512
      medium: 1024
      large: 2048
    cpu:
      small: 250m
      medium: 500m
      large: 1
    disk:
      small: 256
      medium: 1024
      large: 4096

  # namespace to run functions in
  namespace: direktiv-services-direktiv
  ingressClass: contour.ingress.networking.knative.dev

  # -- number of controller replicas
  replicas: 1

  # -- Egress/Ingress network limit for functions if supported by network
  netShape:


  affinity: {}

  # -- extra containers for function controller, e.g. database containers for google cloud or logging
  extraContainersPod: []

  # -- extra volumes for tasks and knative pods
  extraVolumes: []

  # -- extra containers for tasks and knative pods
  extraContainers:
    []
    # - name: cloud-sql-proxy
    #   image: gcr.io/cloudsql-docker/gce-proxy:1.17
    #   command:
    #     - "/cloud_sql_proxy"
    #     - "-instances=mygcpdb=tcp:5432"
    #     - "-ip_address_types=PRIVATE"
    #   securityContext:
    #     runAsNonRoot: true
    #   resources:
    #     requests:
    #       memory: "2Gi"
    #       cpu:    "1"


natsTLS: &natsTLS
  enabled: true
  secretName: direktiv-tls-secret
  cert: server.crt
  key: server.key

nats:
  delay: 15
  tlsCA:
    enabled: true
    secretName: direktiv-tls-secret
  config:
    nats:
      tls: *natsTLS
    jetstream:
      enabled: true
      fileStore:
        enabled: true
        pvc:
          enabled: true
          size: 10Gi
    cluster:
      enabled: true
      tls: *natsTLS

    routeURLs:
      user: direktiv

victoria-logs-single:
  server:
    image:
      tag: v1.15.0-victorialogs
    retentionPeriod: 7d